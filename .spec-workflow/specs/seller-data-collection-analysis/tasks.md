# Tasks Document

## Implementation Tasks

### Phase 1: Foundation - Configuration & Utilities

- [x] 1. データモデル定義
  - File: `modules/storage/models.py`
  - Purpose: Seller/Productのデータクラス定義
  - _Leverage: Python dataclasses（標準ライブラリ）_
  - _Requirements: Requirement 2 (Yahoo Auctionsセラーページからの商品名取得), Requirement 3 (中間CSVエクスポート)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in data modeling and type safety | Task: Create comprehensive dataclass definitions for Seller and Product models following requirements 2 and 3 from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Define type-safe data structures with proper field validation. | Restrictions: Use Python 3.12+ type hints (list[str], bool | None), Do not add methods to dataclasses (pure data containers), Follow naming conventions from structure.md (snake_case for fields), Do not use mutable default values | Leverage: Python dataclasses standard library, Type hints for all fields | Requirements: Requirement 2 (Yahoo Auctionsセラーページからの商品名取得), Requirement 3 (中間CSVエクスポート) | Success: Seller dataclass with fields: seller_name, seller_url, total_price, product_titles, is_anime_seller, Product dataclass with fields: title, seller_name, All fields have proper type annotations, Compiles without type errors (mypy check passes), Follows structure.md naming conventions | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [x] 2. 設定ファイル作成 (constants.py)
  - File: `modules/config/constants.py`
  - Purpose: 定数定義（MAX_PRODUCTS_PER_SELLER, MIN_SELLER_PRICE等）
  - _Leverage: Python標準ライブラリ_
  - _Requirements: Requirement 1 (Rapras集計ページからのセラーリンク取得), Requirement 2 (Yahoo Auctionsセラーページからの商品名取得)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in configuration management | Task: Define application constants following requirements 1 and 2 from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Centralize all magic numbers and configuration values. | Restrictions: Use UPPER_SNAKE_CASE for all constants, Do not include sensitive data (use settings.py for .env values), Group related constants with comments, Do not use dynamic values (constants must be static) | Leverage: Python standard library (no external dependencies) | Requirements: Requirement 1 (Rapras集計ページからのセラーリンク取得), Requirement 2 (Yahoo Auctionsセラーページからの商品名取得) | Success: MAX_PRODUCTS_PER_SELLER = 12, MIN_SELLER_PRICE = 100000, MAX_RETRY_ATTEMPTS = 3, YAHOO_PROXY = "http://164.70.96.2:3128", RAPRAS_BASE_URL = "https://www.rapras.jp", RAPRAS_SUM_ANALYSE_PATH = "/sum_analyse", RETRY_BACKOFF_SECONDS = [1, 2, 4], All constants properly documented with comments | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [x] 3. 環境変数設定ファイル作成 (settings.py)
  - File: `modules/config/settings.py`
  - Purpose: .envから環境変数を読み込み（RAPRAS_USERNAME, RAPRAS_PASSWORD）
  - _Leverage: python-dotenv_
  - _Requirements: Requirement 1 (Rapras集計ページからのセラーリンク取得)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in secure configuration management | Task: Create settings module to load environment variables from .env file following requirement 1 from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Use python-dotenv for secure credential management. | Restrictions: Never hardcode credentials, Provide clear error messages for missing .env values, Do not commit .env to version control (add to .gitignore), Use type hints for all settings | Leverage: python-dotenv library, os.getenv() for fallback values | Requirements: Requirement 1 (Rapras集計ページからのセラーリンク取得) | Success: RAPRAS_USERNAME and RAPRAS_PASSWORD loaded from .env with python-dotenv, Raises clear error if required env vars are missing, .env.example already contains placeholder values, .gitignore already excludes .env, Type hints for all settings variables, Validation for required environment variables | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [x] 4. ロガー設定作成
  - File: `modules/utils/logger.py`
  - Purpose: INFO/WARNING/ERRORレベルのログ設定
  - _Leverage: Python logging標準ライブラリ_
  - _Requirements: Non-Functional Requirements (Usability - Logging)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in logging and observability | Task: Configure structured logging with INFO/WARNING/ERROR levels following Non-Functional Requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Provide Japanese error messages for user-friendliness. | Restrictions: Use Python logging standard library only, Do not use print() for logging, Ensure thread-safe logging, Log format must include timestamp, level, module name | Leverage: Python logging standard library, logging.basicConfig for setup | Requirements: Non-Functional Requirements (Usability - Logging) | Success: get_logger() function returns configured logger, Log format: "[YYYY-MM-DD HH:MM:SS] LEVEL - module - message", INFO level for normal operations, WARNING for recoverable issues, ERROR for critical failures, Japanese error messages supported, Log to both console and file (logs/app.log) | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
### Phase 2: Scraper Layer

- [x] 5. RaprasScraper実装
  - File: `modules/scraper/rapras_scraper.py`
  - Purpose: Rapras集計ページからセラーリンクを取得
  - _Leverage: Playwright (async API), modules.config.settings, modules.utils.logger_
  - _Requirements: Requirement 1 (Rapras集計ページからのセラーリンク取得)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in web scraping and browser automation | Task: Implement RaprasScraper class following requirement 1 from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Use Playwright async API for robust web scraping with username/password authentication. | Restrictions: Must use async/await pattern, Do not bypass authentication (username/password required), Handle network errors gracefully, Follow design.md interface specifications exactly, Do not hardcode selectors (use data-* attributes or stable selectors) | Leverage: Playwright async API, modules.config.settings (RAPRAS_USERNAME, RAPRAS_PASSWORD), modules.utils.logger | Requirements: Requirement 1 (Rapras集計ページからのセラーリンク取得) | Success: RaprasScraper class with __init__(username: str, password: str), async login() method with username/password authentication flow, async fetch_seller_links(start_date, end_date, min_price) method, Returns list[dict] with seller_name, total_price, link, Filters sellers with total_price >= min_price, Raises AuthenticationError on login failure, Comprehensive logging for all operations, Follows design.md interface specifications | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [x] 6. YahooAuctionScraper実装
  - File: `modules/scraper/yahoo_scraper.py`
  - Purpose: Yahoo Auctionsセラーページから商品名を取得
  - _Leverage: Playwright (プロキシ設定), modules.config.constants, modules.utils.logger_
  - _Requirements: Requirement 2 (Yahoo Auctionsセラーページからの商品名取得)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in web scraping and proxy configuration | Task: Implement YahooAuctionScraper class following requirement 2 from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Use Playwright with proxy configuration and exponential backoff retry logic. | Restrictions: Must use proxy for all Yahoo Auctions requests (http://164.70.96.2:3128), Implement exponential backoff (1s, 2s, 4s), Maximum 3 retry attempts, Must use async/await pattern, Follow design.md interface specifications exactly | Leverage: Playwright async API with proxy configuration, modules.config.constants (YAHOO_PROXY, MAX_RETRY_ATTEMPTS, RETRY_BACKOFF_SECONDS), modules.utils.logger | Requirements: Requirement 2 (Yahoo Auctionsセラーページからの商品名取得) | Success: YahooAuctionScraper class with __init__(proxy: str), async fetch_seller_products(seller_url, max_products=12) method, Returns dict with seller_name, seller_url, product_titles, Implements retry logic with exponential backoff, Raises ConnectionError after 3 failed attempts, Logs warnings for products < 12, Proxy correctly configured in Playwright browser context, Follows design.md interface specifications | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
### Phase 3: Analyzer Layer

- [x] 7. AnimeFilter実装
  - File: `modules/analyzer/anime_filter.py`
  - Purpose: Gemini CLIでアニメタイトル判定
  - _Leverage: subprocess (Gemini CLI), modules.utils.logger_
  - _Requirements: Requirement 4 (アニメタイトル判定による二次創作セラー特定)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in external API integration and text processing | Task: Implement AnimeFilter class following requirement 4 from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Use Gemini CLI via subprocess for anime title detection with early termination optimization. | Restrictions: Use subprocess.run() for Gemini CLI execution, Capture both stdout and stderr, Extract only first 2 words from product title, Implement early termination (skip remaining products after first "はい"), Handle Gemini API errors gracefully (continue to next product), Follow design.md interface specifications exactly | Leverage: subprocess standard library, modules.utils.logger, Gemini CLI: gemini -m "gemini-2.5-flash" -p "$PROMPT" | Requirements: Requirement 4 (アニメタイトル判定による二次創作セラー特定) | Success: AnimeFilter class with __init__(model: str = "gemini-2.5-flash"), is_anime_title(title: str) -> bool method, filter_sellers(sellers: list[dict]) -> list[dict] method, Title extraction: split()[:2] for first 2 words, Gemini CLI prompt: "このタイトルはアニメ作品ですか？（タイトル: {title}）", Returns True if response contains "はい" or "アニメ", Early termination implemented (skip after first True), Handles GeminiAPIError gracefully, Logs all API calls and responses, Follows design.md interface specifications | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
### Phase 4: Storage Layer

- [x] 8. CSVExporter実装
  - File: `modules/storage/csv_exporter.py`
  - Purpose: セラーデータをCSV形式でエクスポート
  - _Leverage: Pandas, modules.utils.logger_
  - _Requirements: Requirement 3 (中間CSVエクスポート), Requirement 4 (アニメタイトル判定による二次創作セラー特定)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in data export and file I/O | Task: Implement CSVExporter class following requirements 3 and 4 from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Use Pandas for CSV generation with proper Japanese character encoding. | Restrictions: Use UTF-8 encoding for Japanese characters, Create output directory if not exists, Generate timestamped filenames, Map boolean to Japanese text ("はい"/"いいえ"/"未判定"), Handle IOError gracefully, Follow design.md interface specifications exactly | Leverage: Pandas library (DataFrame.to_csv()), modules.utils.logger, modules.storage.models (Seller dataclass) | Requirements: Requirement 3 (中間CSVエクスポート), Requirement 4 (アニメタイトル判定による二次創作セラー特定) | Success: CSVExporter class with __init__(output_dir: str = "output/"), export_intermediate_csv(sellers: list[dict]) -> str method, export_final_csv(sellers: list[dict]) -> str method, Filename format: sellers_{YYYYMMDD_HHMMSS}.csv and sellers_{YYYYMMDD_HHMMSS}_final.csv, CSV columns: セラー名, セラーページURL, 二次創作, is_anime_seller mapping: True→"はい", False→"いいえ", None→"未判定", Auto-create output/ directory, UTF-8 encoding with BOM for Excel compatibility, Raises IOError on write failure, Follows design.md interface specifications | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
### Phase 5: CLI Layer & Orchestration

- [x] 9. main.py実装 (並行処理オーケストレーション)
  - File: `main.py`
  - Purpose: 全コンポーネントを統合し、並行処理を実行
  - _Leverage: asyncio, modules/scraper/, modules/analyzer/, modules/storage/_
  - _Requirements: Requirement 5 (複数セラー並行処理), All requirements_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in async orchestration and CLI design | Task: Implement main.py CLI entrypoint following requirement 5 and all requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Orchestrate all components with asyncio for parallel processing (max 3 concurrent). | Restrictions: Use asyncio.Semaphore(3) for max 3 concurrent tasks, Handle partial failures (continue processing other sellers), Must follow exact workflow: Rapras login → Fetch seller links → Fetch products → Export intermediate CSV → Anime filter → Export final CSV, Log timeout warning if total time > 5 minutes, Do not block on individual seller failures, Follow design.md architecture exactly | Leverage: asyncio (Semaphore, gather, create_task), modules.scraper.rapras_scraper (RaprasScraper), modules.scraper.yahoo_scraper (YahooAuctionScraper), modules.analyzer.anime_filter (AnimeFilter), modules.storage.csv_exporter (CSVExporter), modules.config.settings (load_rapras_config), modules.utils.logger | Requirements: Requirement 1 (Rapras集計ページからのセラーリンク取得), Requirement 2 (Yahoo Auctionsセラーページからの商品名取得), Requirement 3 (中間CSVエクスポート), Requirement 4 (アニメタイトル判定による二次創作セラー特定), Requirement 5 (複数セラー並行処理) | Success: async main() function with CLI argument parsing, Workflow: Login → Fetch seller links → Parallel fetch products → Intermediate CSV → Anime filter → Final CSV, Max 3 concurrent seller processing (asyncio.Semaphore), asyncio.gather() with return_exceptions=True for partial failure handling, Timeout warning after 5 minutes, Success/failure summary logged at end, CLI arguments: --start-date, --end-date, --min-price (optional), Follows design.md performance optimization strategies, Proper error handling and logging throughout | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
### Phase 6: Testing

- [ ] 10. ユニットテスト作成 (models.py)
  - File: `tests/test_storage/test_models.py`
  - Purpose: Seller/Productデータクラスのテスト
  - _Leverage: pytest, modules.storage.models_
  - _Requirements: Non-Functional Requirements (Testability - 90% coverage)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer specializing in Python unit testing and pytest | Task: Create comprehensive unit tests for data models following Non-Functional Requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Follow structure.md testing standards with Given/When/Then format. | Restrictions: Use pytest framework only, Follow Given/When/Then comment structure, Test normal, boundary, and error cases, Do not test Python standard library behavior, Each test must be independent (no shared state) | Leverage: pytest framework, modules.storage.models (Seller, Product) | Requirements: Non-Functional Requirements (Testability - 90% coverage) | Success: Test normal instantiation of Seller and Product, Test boundary values (empty strings, None, empty lists), Test type validation with invalid types, Test is_anime_seller None/True/False states, All tests use Given/When/Then comments, pytest passes with 100% coverage for models.py, Follows structure.md testing standards | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [x] 11. ユニットテスト作成 (rapras_scraper.py)
  - File: `tests/test_scraper/test_rapras_scraper.py`
  - Purpose: RaprasScraperのモックテスト
  - _Leverage: pytest, pytest-asyncio, pytest-mock, modules.scraper.rapras_scraper_
  - _Requirements: Requirement 1, Non-Functional Requirements (Testability)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer specializing in async testing and web scraping test strategies | Task: Create comprehensive unit tests for RaprasScraper following requirement 1 and Non-Functional Requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Mock Playwright to avoid real network calls. | Restrictions: Use pytest-asyncio for async tests, Mock all Playwright interactions, Do not make real network requests, Follow Given/When/Then comment structure, Test normal, error, and boundary cases, Each test must be independent | Leverage: pytest-asyncio for async test support, pytest-mock for mocking Playwright, modules.scraper.rapras_scraper (RaprasScraper) | Requirements: Requirement 1 (Rapras集計ページからのセラーリンク取得), Non-Functional Requirements (Testability - 90% coverage) | Success: Test successful login flow, Test login failure (AuthenticationError), Test fetch_seller_links with valid data, Test min_price filtering (>= 100000), Test empty seller list, Test boundary: total_price exactly 100000, All Playwright calls mocked, All tests use Given/When/Then comments, pytest passes with 90%+ coverage for rapras_scraper.py | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [ ] 12. ユニットテスト作成 (yahoo_scraper.py)
  - File: `tests/test_scraper/test_yahoo_scraper.py`
  - Purpose: YahooAuctionScraperのリトライロジックテスト
  - _Leverage: pytest, pytest-asyncio, pytest-mock, modules.scraper.yahoo_scraper_
  - _Requirements: Requirement 2, Non-Functional Requirements (Reliability - Retry Strategy)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer specializing in retry logic and error handling testing | Task: Create comprehensive unit tests for YahooAuctionScraper following requirement 2 and Non-Functional Requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Test exponential backoff retry logic thoroughly. | Restrictions: Use pytest-asyncio for async tests, Mock all Playwright interactions, Do not make real network requests, Follow Given/When/Then comment structure, Test retry behavior explicitly (verify backoff timing), Each test must be independent | Leverage: pytest-asyncio for async test support, pytest-mock for mocking Playwright and time.sleep, modules.scraper.yahoo_scraper (YahooAuctionScraper), modules.config.constants (RETRY_BACKOFF_SECONDS) | Requirements: Requirement 2 (Yahoo Auctionsセラーページからの商品名取得), Non-Functional Requirements (Reliability - Retry Strategy) | Success: Test successful product fetch (12 items), Test fetch with < 12 products (warning logged), Test 0 products, Test retry logic (fail 2 times, succeed on 3rd), Test ConnectionError after 3 failed retries, Test exponential backoff timing (1s, 2s, 4s), Test proxy configuration, All Playwright calls mocked, All tests use Given/When/Then comments, pytest passes with 90%+ coverage for yahoo_scraper.py | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [ ] 13. ユニットテスト作成 (anime_filter.py)
  - File: `tests/test_analyzer/test_anime_filter.py`
  - Purpose: AnimeFilterのGemini CLI呼び出しテスト
  - _Leverage: pytest, pytest-mock, modules.analyzer.anime_filter_
  - _Requirements: Requirement 4, Non-Functional Requirements (Testability)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer specializing in subprocess testing and external API mocking | Task: Create comprehensive unit tests for AnimeFilter following requirement 4 and Non-Functional Requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Mock subprocess calls to avoid real Gemini API usage. | Restrictions: Mock subprocess.run() for all tests, Do not call real Gemini API, Follow Given/When/Then comment structure, Test early termination behavior, Test error handling for subprocess failures, Each test must be independent | Leverage: pytest-mock for mocking subprocess, modules.analyzer.anime_filter (AnimeFilter) | Requirements: Requirement 4 (アニメタイトル判定による二次創作セラー特定), Non-Functional Requirements (Testability - 90% coverage) | Success: Test is_anime_title() returns True for "はい" response, Test is_anime_title() returns True for "アニメ" response, Test is_anime_title() returns False for "いいえ" response, Test title extraction (first 2 words from product name), Test filter_sellers() with early termination (skip after first True), Test filter_sellers() with all False (non-anime seller), Test GeminiAPIError handling (continue to next product), Test empty title / 1-word title boundary cases, All subprocess calls mocked, All tests use Given/When/Then comments, pytest passes with 90%+ coverage for anime_filter.py | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [ ] 14. ユニットテスト作成 (csv_exporter.py)
  - File: `tests/test_storage/test_csv_exporter.py`
  - Purpose: CSVExporterのファイル出力テスト
  - _Leverage: pytest, pytest-mock, pandas, modules.storage.csv_exporter_
  - _Requirements: Requirement 3, Requirement 4, Non-Functional Requirements (Testability)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer specializing in file I/O testing and data validation | Task: Create comprehensive unit tests for CSVExporter following requirements 3 and 4 and Non-Functional Requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Verify CSV content, encoding, and file creation. | Restrictions: Use temporary directories for test output (pytest tmp_path fixture), Do not write to project output/ directory, Follow Given/When/Then comment structure, Verify CSV content matches expected format, Test both intermediate and final CSV exports, Each test must be independent | Leverage: pytest tmp_path fixture for temporary directories, pandas.read_csv for verification, modules.storage.csv_exporter (CSVExporter) | Requirements: Requirement 3 (中間CSVエクスポート), Requirement 4 (アニメタイトル判定による二次創作セラー特定), Non-Functional Requirements (Testability - 90% coverage) | Success: Test export_intermediate_csv() with "未判定" for is_anime_seller, Test export_final_csv() with boolean to Japanese mapping (True→"はい", False→"いいえ", None→"未判定"), Test filename format (sellers_{timestamp}.csv), Test CSV columns: セラー名, セラーページURL, 二次創作, Test UTF-8 encoding with BOM, Test directory auto-creation, Test IOError handling (write permission failure), Test empty seller list, All tests use Given/When/Then comments, pytest passes with 90%+ coverage for csv_exporter.py | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [ ] 15. 統合テスト作成 (E2E workflow)
  - File: `tests/integration/test_e2e_workflow.py`
  - Purpose: Rapras → Yahoo Auctions → CSV のフルフローテスト
  - _Leverage: pytest, pytest-asyncio, all modules_
  - _Requirements: All requirements, Non-Functional Requirements (Testability)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer specializing in integration testing and end-to-end workflows | Task: Create end-to-end integration tests following all requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Mock external services but test real module integration. | Restrictions: Mock Rapras/Yahoo Auctions/Gemini CLI (no real network calls), Test actual module integration (not mocked internally), Follow Given/When/Then comment structure, Verify CSV output files, Test parallel processing behavior, Each test must be independent | Leverage: pytest-asyncio for async test support, pytest-mock for external service mocking, All modules (scraper, analyzer, storage), pytest tmp_path for output verification | Requirements: All requirements (Requirement 1-5), Non-Functional Requirements (Testability - 90% coverage) | Success: Test scenario 1: Full success workflow (Rapras login → fetch sellers → fetch products → intermediate CSV → anime filter → final CSV), Test scenario 2: Partial failure (3 sellers fail Yahoo Auctions, 7 succeed), Test scenario 3: Gemini API errors (continue processing), Test parallel processing (max 3 concurrent, using asyncio.Semaphore), Test timeout warning (> 5 minutes), Verify intermediate CSV contains "未判定", Verify final CSV contains "はい"/"いいえ", All external services mocked, All tests use Given/When/Then comments, pytest passes successfully | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
### Phase 7: Quality & Documentation

- [ ] 16. 品質チェック実行 & 修正
  - File: Multiple files (all modules and tests)
  - Purpose: Black, Ruff, pytest, カバレッジ90%達成
  - _Leverage: black, ruff, pytest, pytest-cov, bandit, safety_
  - _Requirements: Non-Functional Requirements (Code Quality, Test Coverage 90%)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Senior Python Developer specializing in code quality and security | Task: Execute all quality checks following tech.md standards and fix any issues to meet Non-Functional Requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Ensure 90% test coverage and zero security vulnerabilities. | Restrictions: Follow exact order from tech.md (Black → Ruff → pytest → Coverage → bandit → safety), Do not delete tests to achieve coverage, Fix all High severity bandit warnings, All pytest tests must pass, Coverage must be >= 90%, Do not skip any quality check steps | Leverage: black (code formatting), ruff (linting and import sorting), pytest (unit testing), pytest-cov (coverage reporting), bandit (security scanning), safety (dependency vulnerability checking) | Requirements: Non-Functional Requirements (Code Quality, Test Coverage 90%) | Success: Step 1: black modules/ tests/ main.py (auto-format applied), Step 2: ruff check --fix modules/ tests/ main.py (all auto-fixable issues resolved), Step 3: pytest tests/ -v (all tests pass), Step 4: pytest --cov=modules --cov-report=html (coverage >= 90%), Step 5: bandit -r modules/ tests/ main.py (no High severity warnings), Step 6: safety check --json (no vulnerabilities), All quality checks pass, Coverage report generated in htmlcov/index.html | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [ ] 17. pyproject.toml & 依存関係設定
  - File: `pyproject.toml`
  - Purpose: プロジェクト設定と依存パッケージ定義
  - _Leverage: uv, Python packaging standards_
  - _Requirements: Non-Functional Requirements (Code Quality, Security)_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python DevOps Engineer specializing in dependency management and project configuration | Task: Create pyproject.toml with all dependencies following tech.md standards and Non-Functional Requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Use uv for package management. | Restrictions: Use uv package manager syntax, Pin major versions for dependencies, Separate dev dependencies (pytest, black, ruff, etc.), Include all quality tools (black, ruff, pytest, bandit, safety), Follow PEP 621 standard | Leverage: uv package manager, Python packaging standards (PEP 621) | Requirements: Non-Functional Requirements (Code Quality, Security) | Success: pyproject.toml with [project] section, Dependencies: playwright, pandas, python-dotenv, Dev dependencies: pytest, pytest-asyncio, pytest-mock, pytest-cov, black, ruff, bandit, safety, Python version requirement: >= 3.12, Project name: yahoo-auction-scraper, Entry point: main.py, Tool configurations for black, ruff, pytest, All dependencies installable with: uv sync | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
- [ ] 18. README.md作成
  - File: `README.md`
  - Purpose: プロジェクト概要、セットアップ手順、使用方法
  - _Leverage: Markdown best practices_
  - _Requirements: All requirements, Non-Functional Requirements_
  - _Prompt: Implement the task for spec seller-data-collection-analysis, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Writer specializing in developer documentation | Task: Create comprehensive README.md following all requirements from .spec-workflow/specs/seller-data-collection-analysis/requirements.md. Provide clear setup instructions and usage examples. | Restrictions: Write in Japanese (project is for Japanese users), Include all setup steps (uv, .env, Playwright), Provide usage examples with real commands, Do not include sensitive data (use placeholders), Follow Markdown best practices, Include troubleshooting section | Leverage: Markdown syntax, All project documentation (.spec-workflow/specs/, design.md) | Requirements: All requirements (Requirement 1-5), Non-Functional Requirements | Success: Project overview in Japanese, Features list matching requirements, Prerequisites: Python 3.12+, uv, Gemini CLI, Setup instructions: 1. Clone repository, 2. uv sync, 3. Create .env with RAPRAS_USERNAME and RAPRAS_PASSWORD, 4. playwright install chromium, Usage examples: python main.py --start-date 2025-08-01 --end-date 2025-10-31, Project structure diagram, Testing instructions (pytest), Quality check instructions (black, ruff, etc.), Troubleshooting section (common errors), License information | Instructions: Before starting implementation, mark this task as in-progress ([-]) in tasks.md. After completing implementation, mark as completed ([x]) in tasks.md._
## Task Summary

**Total Tasks**: 18
- **Phase 1** (Foundation): 4 tasks
- **Phase 2** (Scraper Layer): 2 tasks
- **Phase 3** (Analyzer Layer): 1 task
- **Phase 4** (Storage Layer): 1 task
- **Phase 5** (CLI Layer): 1 task
- **Phase 6** (Testing): 6 tasks
- **Phase 7** (Quality & Documentation): 3 tasks

**Estimated Completion Time**: 10-12 hours (assuming parallel development)

**Critical Path**:
1. Foundation (Tasks 1-4) → Scraper (Tasks 5-6) → Storage (Task 8) → CLI (Task 9) → Testing (Tasks 10-15) → Quality (Tasks 16-18)
2. Analyzer (Task 7) can be developed in parallel with Storage (Task 8)
